{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BbhwsxphhBS"
   },
   "source": [
    "# Finetuning CTC models on other languages\n",
    "\n",
    "In previous tutorials, we have seen a few ways to restore an ASR model, set up the data loaders, and then either train from scratch or fine-tune the model on a small dataset. In this tutorial, we extend previous tutorials and discuss in detail how to * fine-tune a pre-trained model onto a new language*. While many of the concepts are similar to previous tutorials, this tutorial will dive deeper into essential steps. Namely,\n",
    "\n",
    " - Data preprocessing\n",
    " - Prepare tokenizers\n",
    " - Discuss how to fine-tune models on low-resource languages efficiently\n",
    " - Train a character encoding CTC model\n",
    " - Train a sub-word encoding CTC model\n",
    "\n",
    "For this tutorial (and limited by the compute and storage available on Colab environments), we will attempt to fine-tune an English ASR model onto the [Mozilla Common Voice](https://commonvoice.mozilla.org/en) dataset for Japanese. This dataset will also allow us to discuss a few details for fine-tuning low-resource languages. The methods discussed here can also be applied to languages with several thousand hours of data!\n",
    "\n",
    "**Note**: It is advised to review the execution flow diagram for ASR models in order to correctly setup the model prior to fine-tuning - [ASR CTC Examples](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_ctc/README.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cjMaek4rY8-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TSTb6b5DriWG"
   },
   "outputs": [],
   "source": [
    "# Nemo import and Nemo utils\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "from nemo.utils import logging, exp_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset and direcotries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8wqTRjpNruZD"
   },
   "outputs": [],
   "source": [
    "# Make the directory for scripts and datasets\n",
    "data_dir = 'datasets/'\n",
    "\n",
    "if not os.path.exists(\"scripts\"):\n",
    "  os.makedirs(\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eliminazione file: 100%|██████████| 12189/12189 [14:41<00:00, 13.82file/s]\n",
      "Eliminazione file: 100%|██████████| 14549/14549 [16:18<00:00, 14.87file/s]\n"
     ]
    }
   ],
   "source": [
    "# Tolgo i file invalidati del dataset (Serve solo per i Mozilla Common Voice datasets)\n",
    "prepare_dataset.delete_invalidated(\"/workspace/finetuning/datasets/invalidated.tsv\")\n",
    "prepare_dataset.delete_invalidated(\"/workspace/finetuning/datasets/other.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory dei file audio\n",
    "clips_dir = os.path.join('datasets', \"clips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MP3 to WAV: 100%|██████████| 102579/102579 [4:56:38<00:00,  5.76it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversione completata!\n"
     ]
    }
   ],
   "source": [
    "# Il dataset scaricato contiene file .mp3, quindi converto a .wav\n",
    "# Aggiorno anche il path dei file aggiungendo davanti clips_dir\n",
    "prepare_dataset.convert_mp3_to_wav(clips_dir) # This may took some time... (not multiprocessing or some optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendo il nome dei file che contengono i path degli audio\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "validation_file = os.path.join(data_dir, \"dev.csv\")\n",
    "test_file = os.path.join(data_dir, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converto da tsv a csv e tengo solo le colonne rilevanti (audio_path e testo_audio)\n",
    "train_file, test_file, validation_file = prepare_dataset.process_tsv(clips_dir, [train_file, test_file, validation_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset.remove_mp3(clips_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riduco i file da usare perché ho un HW limitato e aggiorno i CSV con le informazioni dei file\n",
    "reduce_dataset = True\n",
    "if reduce_dataset:\n",
    "    prepare_dataset.keep_first_n_lines(train_file, 1000)\n",
    "    prepare_dataset.keep_first_n_lines(test_file, 100)\n",
    "    prepare_dataset.keep_first_n_lines(validation_file, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faccio la conversione dei dataset in un formato compatibile con NeMo e genero i manifest contenti audio_path, sentence e duration del file\n",
    "# Potrebbe settare altre cose di NeMo e fare controlli che sia compatibile (sposta file, controlla integrità ecc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest salvato in: datasets/train/train_manifest.json\n",
      "Manifest salvato in: datasets/validation/validation_manifest.json\n",
      "Manifest salvato in: datasets/test/test_manifest.json\n"
     ]
    }
   ],
   "source": [
    "# Converto i file da MCV to NeMo format\n",
    "prepare_dataset.MVCtoNEMO(train_file, \"train\")\n",
    "prepare_dataset.MVCtoNEMO(validation_file, \"validation\")\n",
    "prepare_dataset.MVCtoNEMO(test_file, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendo il nome dei file che contengono i path degli audio\n",
    "data_dir = \"datasets\"\n",
    "train_file = os.path.join(data_dir, \"train_manifest.json\")\n",
    "validation_file = os.path.join(data_dir, \"validation_manifest.json\")\n",
    "test_file = os.path.join(data_dir, \"test_manifest.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File JSON normalizzato e salvato in: datasets/train_manifest.json\n",
      "File JSON normalizzato e salvato in: datasets/validation_manifest.json\n",
      "File JSON normalizzato e salvato in: datasets/test_manifest.json\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset.normalize_text(train_file)\n",
    "prepare_dataset.normalize_text(validation_file)\n",
    "prepare_dataset.normalize_text(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def reorder_and_rename_columns(json_file):\n",
    "    # Legge il file JSON in un DataFrame\n",
    "    df = pd.read_json(json_file, lines=True)\n",
    "\n",
    "    # Rinomina la colonna \"audio\" in \"audio_filepath\"\n",
    "    df = df.rename(columns={\"audio\": \"audio_filepath\"})\n",
    "    \n",
    "    # Riordina le colonne\n",
    "    ordered_df = df[['text', 'audio_filepath', 'duration']]\n",
    "\n",
    "    # Salva il DataFrame con le colonne rinominate e riordinate nel file JSON\n",
    "    ordered_df.to_json(json_file, orient=\"records\", lines=True, force_ascii=False)\n",
    "    print(f\"File JSON con colonne rinominate e riordinate salvato in: {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File JSON con colonne rinominate e riordinate salvato in: datasets/train_manifest.json\n",
      "File JSON con colonne rinominate e riordinate salvato in: datasets/test_manifest.json\n",
      "File JSON con colonne rinominate e riordinate salvato in: datasets/validation_manifest.json\n"
     ]
    }
   ],
   "source": [
    "reorder_and_rename_columns(train_file)\n",
    "reorder_and_rename_columns(test_file)\n",
    "reorder_and_rename_columns(validation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Downloaded from https://commonvoice.mozilla.org/it/datasets the dataset 'Common Voice Corpus 3' of 1 GB. In realtà è solo una piccola parte di un dataset. MVC rilascia dataset interi e piccoli aggiornamenti chiamati Delta Segment, io ho installato uno di questi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-wI16qY_misb"
   },
   "outputs": [],
   "source": [
    "LANGUAGE = \"it\"\n",
    "tokenizer_dir = os.path.join('tokenizers', LANGUAGE) \n",
    "manifest_dir = data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYw2sHWtOh3x"
   },
   "source": [
    "Now that the dataset has been downloaded, let's prepare some paths to easily access the manifest files for the train, dev, and test partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_LhoUuDrmHb"
   },
   "source": [
    "# Preparing the manifest\n",
    "\n",
    "Before we start training the model on the above unprocessed manifest files, we need to analyze the data. Data pre-processing is perhaps the most essential task, and often requires moderate expertise in the language.\n",
    "\n",
    "While we could technically use the manifests above to train a model, the results would potentially be abysmal. Let's dive a little deeper into what challenges this dataset poses to our models.\n",
    "\n",
    "**Note**: The pre-processing done on this corpus is specifically done to reduce ambiguity in transcripts, due to the minuscule amount of data we possess. Given enough data, the models discussed here could potentially learn well, even without such heavy pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7WAGLX59C26"
   },
   "outputs": [],
   "source": [
    "# Salvo il path dei manifest creati con lo script NVIDIA\n",
    "train_manifest = f\"{manifest_dir}/train/train/train_manifest.json\"\n",
    "dev_manifest = f\"{manifest_dir}/validation/validation/validation_manifest.json\"\n",
    "test_manifest = f\"{manifest_dir}/test/test/test_manifest.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txKWXZLbrUsU"
   },
   "source": [
    "### Manifest utilities\n",
    "\n",
    "First, we construct some utilities to read and write manifest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdkJYxUirp7C"
   },
   "outputs": [],
   "source": [
    "# Manifest Utils\n",
    "from tqdm.auto import tqdm\n",
    "from nemo.collections.asr.parts.utils.manifest_utils import read_manifest, write_manifest\n",
    "import json\n",
    "\n",
    "def write_processed_manifest(data, original_path):\n",
    "    original_manifest_name = os.path.basename(original_path)\n",
    "    new_manifest_name = original_manifest_name.replace(\".json\", \"_processed.json\")\n",
    "\n",
    "    manifest_dir = os.path.split(original_path)[0]\n",
    "    filepath = os.path.join(manifest_dir, new_manifest_name)\n",
    "    write_manifest(filepath, data)\n",
    "    print(f\"Finished writing manifest: {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HngfzcwOijy4"
   },
   "outputs": [],
   "source": [
    "# Leggo i manifest e salvo il contenuto\n",
    "train_manifest_data = read_manifest(train_manifest)\n",
    "dev_manifest_data = read_manifest(dev_manifest)\n",
    "test_manifest_data = read_manifest(test_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thrTwcCVra2N"
   },
   "source": [
    "Next, we extract just the text corpus from the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2iwnvhXimfG"
   },
   "outputs": [],
   "source": [
    "# Estraggo il testo dal manifest per poterne valutare i caratteri\n",
    "train_text = [data['text'] for data in train_manifest_data]\n",
    "dev_text = [data['text'] for data in dev_manifest_data]\n",
    "test_text = [data['text'] for data in test_manifest_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tengo solo poche righe per limiti HW\n",
    "if reduce_dataset:\n",
    "    prepare_dataset.keep_first_n_lines(train_manifest, 1000)\n",
    "    prepare_dataset.keep_first_n_lines(test_manifest, 100)\n",
    "    prepare_dataset.keep_first_n_lines(dev_manifest, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fDoQQwyrhkV"
   },
   "source": [
    "## Character Processing\n",
    "\n",
    "Let us calculate the character set - which is the set of unique tokens that exist within the text manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpUb_pI5imhh"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "# Ritorna i caratteri contenuti nel manifest manifest_data\n",
    "def get_charset(manifest_data):\n",
    "    charset = defaultdict(int)\n",
    "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
    "        text = row['text']\n",
    "        for character in text:\n",
    "            charset[character] += 1\n",
    "    return charset\n",
    "\n",
    "# Stampa le lettere mancanti dal set dato in input\n",
    "def lettere_mancanti(dizionario):\n",
    "    alfabeto = set(string.ascii_letters)\n",
    "    lettere_presenti = set(dizionario.keys())\n",
    "    lettere_mancanti = alfabeto - lettere_presenti\n",
    "    return sorted(lettere_mancanti)\n",
    "\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\…\\{\\}\\(\\)\\[\\]\\–\\’\\'\\/]'  # Caratteri da ignorare\n",
    "\n",
    "# Rimuove i caratteri di chars_to_ignore_regex\n",
    "def remove_special_characters(data):\n",
    "    data[\"text\"] = re.sub(chars_to_ignore_regex, '', data[\"text\"]).lower().strip()\n",
    "    return data\n",
    "\n",
    "# Applica i filtri sui caratteri \n",
    "def apply_preprocessors(manifest, preprocessors):\n",
    "    for processor in preprocessors:\n",
    "        for idx in tqdm(range(len(manifest)), desc=f\"Applying {processor.__name__}\"):\n",
    "            manifest[idx] = processor(manifest[idx])\n",
    "\n",
    "    print(\"Finished processing manifest !\")\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottengo i set di caratteri di ogni set\n",
    "train_charset = get_charset(train_manifest_data)\n",
    "dev_charset = get_charset(dev_manifest_data)\n",
    "test_charset = get_charset(test_manifest_data)\n",
    "\n",
    "# Stampo le lettere mancanti (dell'alfabeto) di ogni set (differenza tra maiuscole e minuscole come con ASCII)\n",
    "print(\"Lettere Mancanti da: train, validation e test\")\n",
    "print(lettere_mancanti(train_charset))\n",
    "print(lettere_mancanti(dev_charset))\n",
    "print(lettere_mancanti(test_charset))\n",
    "print()\n",
    "\n",
    "# Conto il numero di caratteri unici nei tre split\n",
    "train_dev_set = set.union(set(train_charset.keys()), set(dev_charset.keys()))\n",
    "test_set = set(test_charset.keys())\n",
    "print(f\"Numero di caratteri nel train+dev set : {len(train_dev_set)}\")\n",
    "print(f\"Numero di caratteri nel test set : {len(test_set)}\")\n",
    "print()\n",
    "\n",
    "# List of pre-processing functions\n",
    "PREPROCESSORS = [\n",
    "    remove_special_characters\n",
    "    # other functions for complex languages\n",
    "]\n",
    "\n",
    "# Load manifests\n",
    "print(\"Leggo i manifest...\")\n",
    "train_data = read_manifest(train_manifest)\n",
    "dev_data = read_manifest(dev_manifest)\n",
    "test_data = read_manifest(test_manifest)\n",
    "print()\n",
    "\n",
    "# Applico i filtri inseriti in PREPROCESSORS\n",
    "print(\"Applico i filtri...\")\n",
    "train_data_processed = apply_preprocessors(train_data, PREPROCESSORS)\n",
    "dev_data_processed = apply_preprocessors(dev_data, PREPROCESSORS)\n",
    "test_data_processed = apply_preprocessors(test_data, PREPROCESSORS)\n",
    "print()\n",
    "\n",
    "# Aggiorno i manifest con i filtri applicati\n",
    "print(\"Aggiorno i manifest...\")\n",
    "train_manifest_cleaned = write_processed_manifest(train_data_processed, train_manifest)\n",
    "dev_manifest_cleaned = write_processed_manifest(dev_data_processed, dev_manifest)\n",
    "test_manifest_cleaned = write_processed_manifest(test_data_processed, test_manifest)\n",
    "print()\n",
    "\n",
    "# Aggiorno i set di caratteri modificati \n",
    "print(\"Rileggo i caratteri aggionati...\")\n",
    "train_manifest_data = read_manifest(train_manifest_cleaned)\n",
    "train_charset = get_charset(train_manifest_data)\n",
    "\n",
    "dev_manifest_data = read_manifest(dev_manifest_cleaned)\n",
    "dev_charset = get_charset(dev_manifest_data)\n",
    "\n",
    "test_manifest_data = read_manifest(test_manifest_cleaned)\n",
    "test_charset = get_charset(test_manifest_data)\n",
    "print()\n",
    "\n",
    "# Aggiorno il nuovo set di caratteri finale di test e validation\n",
    "train_dev_set = sorted(set.union(set(train_charset.keys()), set(dev_charset.keys())))\n",
    "test_set = set(train_charset.keys())\n",
    "\n",
    "vocaboulary = \" \".join(train_dev_set)\n",
    "print(\"Nuovo character set: \" + vocaboulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnmVqx8aegwR"
   },
   "source": [
    "# Character Encoding CTC Model\n",
    "\n",
    "Now that we have a processed dataset, we can begin training an ASR model on this dataset. The following section will detail how we prepare a CTC model which utilizes a Character Encoding scheme.\n",
    "\n",
    "This section will utilize a pre-trained [QuartzNet 15x5](https://arxiv.org/abs/1910.10261), which has been trained on roughly 7,000 hours of English speech base model. We will modify the decoder layer (thereby changing the model's vocabulary) and then train for a small number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlJmwh-iei77"
   },
   "outputs": [],
   "source": [
    "# Scarico il modello inglese per cui devo fare fine-tuning\n",
    "char_model = nemo_asr.models.ASRModel.from_pretrained(\"stt_en_quartznet15x5\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSI6t9dgSOxj"
   },
   "source": [
    "### Update the vocabulary\n",
    "\n",
    "Changing the vocabulary of a character encoding ASR model is as simple as passing the list of new tokens that comprise the vocabulary as input to `change_vocabulary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VU-jfYLei9-"
   },
   "outputs": [],
   "source": [
    "# Cambio il vocabolario delle lettere\n",
    "#char_model.change_vocabulary(new_vocabulary=list(train_dev_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocco l'encoder del Transformer\n",
    "freeze_encoder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def enable_bn_se(m):\n",
    "    if type(m) == nn.BatchNorm1d:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    if 'SqueezeExcite' in type(m).__name__:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "if freeze_encoder:\n",
    "  char_model.encoder.freeze()\n",
    "  char_model.encoder.apply(enable_bn_se)\n",
    "  logging.info(\"Model encoder has been frozen, and batch normalization has been unfrozen\")\n",
    "else:\n",
    "  char_model.encoder.unfreeze()\n",
    "  logging.info(\"Model encoder has been un-frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_cAiuXSfRdW"
   },
   "source": [
    "### Update config\n",
    "\n",
    "Each NeMo model has a config embedded in it, which can be accessed via `model.cfg`. In general, this is the config that was used to construct the model.\n",
    "\n",
    "For pre-trained models, this config generally represents the config used to construct the model when it was trained. A nice benefit to this embedded config is that we can repurpose it to set up new data loaders, optimizers, schedulers, and even data augmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBIy8p0fV7sa"
   },
   "outputs": [],
   "source": [
    "# Aggiorno le label del modello\n",
    "char_model.cfg.labels = list(train_dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una copia delle configuazioni del modello per poterle modificare\n",
    "cfg = copy.deepcopy(char_model.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBFvo0UUfcuY"
   },
   "source": [
    "### Setting up data loaders\n",
    "\n",
    "Now that the model's character set has been updated let's prepare the model to utilize the new character set even in the data loaders. Note that this is crucial so that the data produced during training/validation matches the new character set, and tokens are encoded/decoded correctly.\n",
    "\n",
    "**Note**: An important config parameter is `normalize_transcripts` and `parser`. There are some parsers that are used for specific languages for character based models - currently only `en` is supported. These parsers will preprocess the text with the given languages parser. However, for other languages, it is advised to explicitly set `normalize_transcripts = False` - which will prevent the parser from processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Converte DictConfig in un dizionario Python\n",
    "cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "\n",
    "# Stampa in formato JSON con indentazione\n",
    "print(json.dumps(cfg_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlQ5iGrZejKy"
   },
   "outputs": [],
   "source": [
    "# Setup train, validation, test configs\n",
    "with open_dict(cfg):\n",
    "  # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
    "  cfg.train_ds.manifest_filepath = f\"{train_manifest_cleaned},{dev_manifest_cleaned}\"\n",
    "  cfg.train_ds.labels = list(train_dev_set)\n",
    "  cfg.train_ds.normalize_transcripts = False\n",
    "  cfg.train_ds.batch_size = 8#32\n",
    "  cfg.train_ds.num_workers = 0#4#8\n",
    "  cfg.train_ds.pin_memory = True\n",
    "  cfg.train_ds.trim_silence = True\n",
    "\n",
    "  # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
    "  cfg.validation_ds.manifest_filepath = test_manifest_cleaned\n",
    "  cfg.validation_ds.labels = list(train_dev_set)\n",
    "  cfg.validation_ds.normalize_transcripts = False\n",
    "  cfg.validation_ds.batch_size = 4#8\n",
    "  cfg.validation_ds.num_workers = 0#4#8\n",
    "  cfg.validation_ds.pin_memory = True\n",
    "  cfg.validation_ds.trim_silence = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx9DixV0ejMo"
   },
   "outputs": [],
   "source": [
    "# setup data loaders with new configs\n",
    "char_model.setup_training_data(cfg.train_ds)\n",
    "char_model.setup_multiple_validation_data(cfg.validation_ds)\n",
    "# Mi dice 2000 (train + validation) e 1000 di test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJc7DyEUfem2"
   },
   "source": [
    "### Setting up optimizer and scheduler\n",
    "\n",
    "When fine-tuning character models, it is generally advised to use a lower learning rate and reduced warmup. A reduced learning rate helps preserve the pre-trained weights of the encoder. Since the fine-tuning dataset is generally smaller than the original training dataset, the warmup steps would be far too much for the smaller fine-tuning dataset.\n",
    "\n",
    "-----\n",
    "**Note**: When freezing the encoder, it is possible to use the original learning rate as the model was trained on. The original learning rate can be used because the encoder is frozen, so the learning rate is used only to optimize the decoder. However, a very high learning rate would still destabilize training, even with a frozen encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgoD5hOKYSKJ"
   },
   "outputs": [],
   "source": [
    "# Original optimizer + scheduler\n",
    "print(OmegaConf.to_yaml(char_model.cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okytaslHejOm"
   },
   "outputs": [],
   "source": [
    "#with open_dict(char_model.cfg.optim):\n",
    "#  char_model.cfg.optim.lr = 0.01\n",
    "#  char_model.cfg.optim.betas = [0.95, 0.5]  # from paper\n",
    "#  char_model.cfg.optim.weight_decay = 0.001  # Original weight decay\n",
    "#  char_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
    "#  char_model.cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n",
    "#  char_model.cfg.optim.sched.min_lr = 1e-3#1e-5\n",
    "with open_dict(cfg.optim):\n",
    "  cfg.optim.lr = 0.01\n",
    "  cfg.optim.betas = [0.95, 0.5]  # from paper\n",
    "  cfg.optim.weight_decay = 0.001  # Original weight decay\n",
    "  cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
    "  cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n",
    "  cfg.optim.sched.min_lr = 1e-5\n",
    "  # ho trovato anche una  versione con 1e-3 ma era con un altro modello e altri settings (usava tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waz64_NXfkIQ"
   },
   "source": [
    "### Setting up augmentation\n",
    "\n",
    "Remember that the model was trained on several thousands of hours of data, so the regularization provided to it might not suit the current dataset. We can easily change it as we see fit.\n",
    "\n",
    "-----\n",
    "\n",
    "You might notice that we utilize `char_model.from_config_dict()` to create a new SpectrogramAugmentation object and assign it directly in place of the previous augmentation. This is generally the syntax to be followed whenever you notice a `_target_` tag in the config of a model's inner config.\n",
    "\n",
    "-----\n",
    "**Note**: For low resource languages, it might be better to increase augmentation via SpecAugment to reduce overfitting. However, this might, in turn, make it too hard for the model to train in a short number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ6Md-dLejRA"
   },
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(char_model.cfg.spec_augment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For low resource languages, it might be better to increase augmentation via SpecAugment to reduce overfitting. However, this might, in turn, make it too hard for the model to train in a short number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ei9WsLzejTI"
   },
   "outputs": [],
   "source": [
    "# Ha senso scommentare solo se alziamo il nuemero di EPOCH\n",
    "#with open_dict(char_model.cfg.spec_augment):\n",
    "#    char_model.cfg.spec_augment.freq_masks = 2      # Numero di maschere di frequenza\n",
    "#    char_model.cfg.spec_augment.freq_width = 25     # Ampiezza massima delle bande di frequenza mascherate\n",
    "#    char_model.cfg.spec_augment.time_masks = 2      # Numero di maschere temporali\n",
    "#    char_model.cfg.spec_augment.time_width = 0.05   # Percentuale del tempo totale mascherato\n",
    "\n",
    "char_model.spec_augmentation = char_model.from_config_dict(char_model.cfg.spec_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK-RQXEZfq1V"
   },
   "source": [
    "### Setup Metrics\n",
    "\n",
    "Originally, the model was trained on an English dataset corpus. When calculating Word Error Rate, we can easily use the \"space\" token as a separator for word boundaries. On the other hand, certain languages such as Japanese and Mandarin do not use \"space\" tokens, instead opting for different ways to annotate the end of the word.\n",
    "\n",
    "In cases where the \"space\" token is not used to denote a word boundary, we can use the Character Error Rate metric instead, which computes the edit distance at a token level rather than a word level.\n",
    "\n",
    "We might also be interested in noting model predictions during training and inference. As such, we can enable logging of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cN1FC0o2ejVg"
   },
   "outputs": [],
   "source": [
    "use_cer = False # tiene gli spazi bianchi come separatori tra parole\n",
    "log_prediction = True\n",
    "char_model.wer.use_cer = use_cer\n",
    "char_model.wer.log_prediction = log_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rizGRfrHf92O"
   },
   "source": [
    "## Setup Trainer and Experiment Manager\n",
    "\n",
    "And that's it! Now we can train the model by simply using the Pytorch Lightning Trainer and NeMo Experiment Manager as always.\n",
    "\n",
    "For demonstration purposes, the number of epochs is kept intentionally low. Reasonable results can be obtained in around 100 epochs (approximately 25 minutes on Colab GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaw1qsQIf1Zv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as ptl\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  accelerator = 'gpu'\n",
    "else:\n",
    "  accelerator = 'cpu'\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "trainer = ptl.Trainer(devices=1,\n",
    "                      accelerator=accelerator,\n",
    "                      max_epochs=EPOCHS,\n",
    "                      accumulate_grad_batches=1,\n",
    "                      enable_checkpointing=False,\n",
    "                      logger=True,\n",
    "                      log_every_n_steps=5,\n",
    "                      check_val_every_n_epoch=10)\n",
    "\n",
    "# Setup model with the trainer\n",
    "char_model.set_trainer(trainer)\n",
    "\n",
    "# Finally, update the model's internal config\n",
    "char_model.cfg = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"Model-{LANGUAGE}.nemo\"\n",
    "char_model.save_to(f\"{save_path}\")\n",
    "print(f\"Model saved at path : {os.getcwd() + os.path.sep + save_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
